Swapless design

The general idea of the swapless design is simple enough: to delete, promote the sibling to parent.  It's the edge cases that make it more complex.  
Also additions are the same; add on the bottom right, making the largest trees you can.

= = = = = = = = = = = = = = = = = = = = = = 

1 deletion example.  Start with 8 leaves in 1 tree.

14
|---------------\
12              13
|-------\       |-------\
08      09      10      11
|---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07

Delete 03.

14
|---------------\
12              13
|-------\       |-------\
08      09      10      11
|---\   |---\   |---\   |---\
00  01  02  XX  04  05  06  07

Promote 02 to the position of 09

14
|---------------\
12              13
|-------\       |-------\
08      02      10      11
|---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07

Recompute 12 and 14

14*
|---------------\
12*             13
|-------\       |-------\
08      02      10      11
|---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07

Done.  Note that we don't have to actually delete 02 *or 03* from the physical tree.  As long as 02 has moved up, and 12 recomputed as the parent of 08 and 02, 03 can no longer be proven and so is logically deleted.

= = = = = = = = = = = = = = = = = = = = = = 

Delete a subtree, moving entire subtrees up. 

Start with 8 leaves.

14
|---------------\
12              13
|-------\       |-------\
08      09      10      11
|---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07

Delete 00 through 03

14
|---------------\
12              13
|-------\       |-------\
08      09      10      11
|---\   |---\   |---\   |---\
XX  XX  XX  XX  04  05  06  07

... which really just means deleting 12, ( delToRaise(0,1,2,3) will return 12 up 1.)  So we can look at it as deleting 12

14
|---------------\
XX              13
|-------\       |-------\
08      09      10      11
|---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07

That means raising 13 up, as well as the entire 13 subtree.  This is best acheived top down, first writing 13 to 14, then 10,11 to 12,13, then 04...07 to 08...11

13
|---------------\
10              11
|-------\       |-------\
04      05      06      07
|---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07

In this case no hashing needs to happen.  The top hash that got promoted (13) became a root, so there's nothing to hash above it.  Again we can leave junk data on the bottom row as it will never be read from.

= = = = = = = = = = = = = = = = = = = = = = 

Tricky cases: delete a whole subtree, then add.  Start with 13 leaves in this configuration:

28
|---------------\
24              25              26
|-------\       |-------\       |-------\
16      17      18      19      20      21      22
|---\   |---\   |---\   |---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07  --  --  --  --  12  13  14

Delete 20 and 21.
 
28
|---------------\
24              25              --
|-------\       |-------\       |-------\
16      17      18      19      --      --      22
|---\   |---\   |---\   |---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07  --  --  --  --  12  13  14

The 26 tree remains, with 26 as a dummy root.  Now add leaf 15.

30
|-------------------------------\
28                              29
|---------------\               |---------------\
24              25              --              27
|-------\       |-------\       |-------\       |-------\
16      17      18      19      --      --      22      23
|---\   |---\   |---\   |---\   |---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07  --  --  --  --  12  13  14  15

When we add 15, 23 and 27 are computed.  When we get to 29, we can't compute 29 as the left child is empty as it was a dummy root.  Instead 27 is promoted to 29 as if 26 had just been deleted.  Deferred deletion is another way to think about dummy roots.

30
|-------------------------------\
28                              27
|---------------\               |---------------\
24              25              22              23
|-------\       |-------\       |-------\       |-------\
16      17      18      19      12      13      14      15
|---\   |---\   |---\   |---\   |---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07  --  --  --  --  12  13  14  15


= = = = = = = = = = = = = = = = = = = = = = 

Deletion starts with a list of locations to delete.  The delToRise() function can take that and turn it into a list of location:rows tuples, where you need to raise every location by the corresponding number of rows.

30
|-------------------------------\
28                              29
|---------------\               |---------------\
24*             25*             26              27
|-------\       |-------\       |-------\       |-------\
16      17      18      19      20      21      22      23
|---\   |---\   |---\   |---\   |---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07  08* 09* 10* 11  12  13  14  15

Say we get a deletion list of 8, 9, 10, 24, 25.

As a first step we de-twin, and get
10, 20, 28.

Should the output then be 11:2, 29:1?

You could raise 11 up 2 rows to 26.  But then you move it again to 28 when 29 moves up 1 row to 30.

But I think we actually want to say 11:2, 29:1.  You're going to have to move things twice.

If instead we just give a source:dest tuple... we'd have more ambiguity.  11:29, 29:30?  But to do you need to move 11 up to 26, then recompute what 29 is, and then move it to 30.  Or set up "dirty" flags somehow.

But really, 11:2, 29:1 doesn't mean just move those two locations.  When you say 29:1, you have to move 29 and everything below it up 1.  with 11:2 that doesn't apply because nothing is under 11.  But 29:1 really means:

8,9,10,11,12,13,14,15,20,23,22,23,26,27 all go up.  And in weird ways; 8 goes to 16, 9 to 17, 15 to 23, 20 to 24, and so on.

Probably should go top down;  First move 29 to 30.  Then 26 to 28, 27 to 29, then 20..23 to 24..27.  For forest construction you can do ranges which is pretty fast.  For pollard it does seem like you only need to move on thing and it carries everything underneath with it.

You should do single raise operations top down, but you need to go through the raise operations in bottom up order; if you do 29:1 before 11:2, you'll move 11 to 26 and mess things up.  You could also *apply* the 29:1 to the 11:2 and transform it into 19:2, but I would prefer to avoid that.  But is something like that the most efficient?  Maybe... you really don't want to be moving things more than you need to.  11 direct to 28 and then you don't touch anything below 28. 

In that case it's 

11->28
29->30, 27*->29 
the * means take everything below with you.  Then flag 30 as dirty and recompute 30 from the new 28, 29; I think that's the only hash needed.


But this also seems good enough:
move 11 to 26.  Compute a new 29 from the new 26 and 27.
Then move 29 up to 30.  (no hash operation as it's a root)
move 26, 27 -> 28, 29
20..23 -> 24..27 but 20,21 are empty, so 22,23->26,27
12..15->20..23.



= = = = = = = = = = = = = = = = = = = = = = 

Addressing.  When we insert leaves into the accumulator, we also commit to their leaf number, which stays stuck in the leaf data until it is deleted.  Before any deletions, there's a nice onitial property where the leaf number tells you how to get to the leaf.  For example, in the tree with 16 leaves:

30
|-------------------------------\
28                              29
|---------------\               |---------------\
24              25              26              27
|-------\       |-------\       |-------\       |-------\
16      17      18      19      20      21      22      23
|---\   |---\   |---\   |---\   |---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15

If we want to go to 00, it's left left left left, or 0000.  To get to 15 it's right right right right, or 1111.  We can enforce this property to make things are where they're supposed to be as this gives some additional security against hash collisions.  But how much?  In the above tree, where can leaf 00 go?

00 can only move up, to 16, 24, 28, and 30.  Likewise here 15 seems to only be able to move up to 23, 27, 29, 30.

But in the middle a lot of things can happen.  5 can go all over the place.  With 1 deletion, 05 can get to 18 (delete 04), 19 (delete 19), 17 (24) or 21 (29).  From there it can get to all of row 2 (24...27).  So maybe the leaf position doesn't tell much at all.  Next step: quantify exactly how many possible places things can move to. 

It seems that 15 is only restricted to the 23->27->29 edge because that's the edge of the tree.  If it were a tree with 32 leaves, 15 could move up to 39 (parent) or 47 (15 right on row 1 which starts at 32).  With a large leftmost tree, things may be able to move around a lot.  They probably saturate at higher rows, but what's the branching factor at lower rows?

= = = = = = = = = = = = = = = = = = = = = = 

Better forest deletion: overwrite parent (revised)

The weird part here is you never actually delete anything.  But it works better.


30
|-------------------------------\
28                              29
|---------------\               |---------------\
24              25              26              27
|-------\       |-------\       |-------\       |-------\
16      17      18      19      20      21      22      23
|---\   |---\   |---\   |---\   |---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15

To delete 9, read 8 and write 8 to 20.

Then to delete 8, read 9, see that it's empty, go up and zero out 20, and read 21 instead, write it to 26.  Now 26 has 21, 20 has 8.
But don't have to move any subtrees; previously we'd have to move 21 up to 26 and 10 up to 20.  We can leave 10 and 11.

30
|-------------------------------\
28                              29
|---------------\               |---------------\
24              25              21 (pos 26)     27
|-------\       |-------\       |-------\       |-------\
16      17      18      19      xx      21      22      23
|---\   |---\   |---\   |---\   |---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07  xx  xx  10  11  12  13  14  15

This is what it looks like after deleting 8 and 9.  Everything works OK with this physical data layout.  The only change is when making proofs, if you see an empty node, just skip it.  
So if we want to prove 10:
11 - ok.  20 - empty, skip.  27 - ok.  28 - ok.  So the proof for 10 is (11, 27, 28).  This is the same as if we had moved the 21 subtree up.
When something changes underneath 27 (say deleting 14) and 29 needs to be recomputed, 29's children are all set, just grab and hash.  That would not be the case if you tried to recompute position 26.  But that won't happen.  The only thing that would cause recomputation of 26 would be deletiong of 10 or 11.  Say we try to delete 11 -- zero out 11, read 10, 


multi deletion: condense (00, 01, 02, 03, 04 -> 24, 04)


then for each deletion:
	write sibling to parent.
	While either aunt* or sibling are empty, keep doing this:
	Rise. Write non-empty sibling to parent, otherwise write self to parent.
	*(An aunt outside of the forest counts as non-empty.)


(it could be fancier / faster if you zero out everything first.  For example, deleting (9, 21)
deletion of  9 checks 8, exists.  up to 21, exists, writes 8 to 20.
deletion of 21 checks 20, exists.  up to 27, exists, writes 20 to 26.

The value at 9 ends up getting written, read back, and written again.  It would be faster if 9 moved all the way up to 26 in one step.  That's a //TODO improvement however as the code will be more complex.

= = = = = = = = = = = = = = = = = = = = = = 
revision:

What we actually need to do is don't delete anything ever.  So there will be no empty locations in the tree at all.  Instead, to see if something is logically empty we can look at its parent.  If a parent is the same as a sibling, you know that position is logically not there, even if there's data there.  Similarly, "do I have a sibling" is now "am I the same as my parent".  Or really, you always, logically, have a sibling.

This is a little less obvious when looking at the on-disk data, but not much.  And it's not any more i/o.  But it makes undo super easy.  All the "undo data" that you need now is just the leaf positions that were deleted.  If you never clear out your position map, then you don't even need that, and can figure out the positions from the rev00.dat files.  To apply undo, you don't actually move any data, you just mark parents of locations as dirty.  For example:

Start with this tree:

14
|---------------\
12              13
|-------\       |-------\
08      09      10      11
|---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07

Delete 4, 5, 6 (condensed to 6, 10):

14
|---------------\
12              07
|-------\       |-------\
08      09      10      07
|---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07

The root 14 is now the hash of (12, 07) which is what we want.  But 4, 5, 6 are still there at the bottom.  The undo data that we save is just (4, 5, 6).  Heck you can probably save (6, 10) and write an uncondense function.  Or really, if you care about size, gzip it because it'll be a bunch of uint64s that are all clustered together and probably gzips down 90% or so.

Now to undo, just mark 10, 11 as dirty, and rehash.

14
|---------------\
12              13
|-------\       |-------\
08      09      10      11
|---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07

Back to how it was.



= = = = = = = = = = = = = = = = = = = = = = 
Further along

Need a word for a child which is the same as its parent.  Clone I guess.  If we're going with this whole geneology metaphor.


Here's a problem:

14
|---------------\
12              13
|-------\       |-------\
08      09      10      11
|---\   |---\   |---\   |---\
00  01  02  03  04  05  06  07

Delete 0, 1, 2, 3, 4.
0,1,2,3 condenses into 12.  So you have 4, 12.  
4 means move 5 to 10.  Then mark 13 as dirty.
12 means move 13 to 14, and mark anything above as dirty (none in this small tree).

You don't want to hash 13 though, that dirt is incorrect.  So we really can't do branch at a time, we have to do row at a time.
You *could* hash 12, 13 and write it to 14, and then right after overwrite 14 with 13, but that's inefficient.

For each individual deletion, (not thinking about batches yet) there are 3 phases:

1 bottom to logical leaf
2 promotion
3 promotion to root

Phase 1 is doing nothing, rising until the position is non-clone.
Phase 2 is sibling promotion.
Phase 3 is promotion and hashing up to the root.  When a node *was* a clone, that node is promoted.  When a node is non-clone, the parent is hashed.

One tricky part is that there are 2 data sources: deletions and which parent/child pairs are identical.  While the old transform wasn't data dependent, this one is.  Without any clones, phase 1 is skipped, and phase 3 is just hashing up to the root.  Easy!
With clones though, we need move deletions up in phase 1, and while you follow the same branch locations in phase 3 regardless of what data is in them, the action taken at each node depends on its clone-ness.

